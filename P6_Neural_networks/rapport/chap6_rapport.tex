% -*- coding: UTF-8; -*-
\documentclass[11pt,onecolumn]{article}
%Pour langue et caractères spéciaux
\usepackage[french]{babel} 
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[latin1]{inputenc}

\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm, columnsep=20pt]{geometry}   %Pour ajuster les marges
\usepackage{graphicx}      %pour inclure des graphiques
\usepackage{url}           %Pour inclure des adresse web
\usepackage{verbatim}      %pour inclure les codes en annexe, on utilisera la commande verbatim
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{xfrac}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

\renewcommand{\baselinestretch}{1.5} 

\begin{document}

%Page titre
\begin{titlepage}
\center

\vspace*{2cm}

\textsc{\LARGE Université de Montréal}\\[1cm] 
\textsc{\Large PHY 3075 -- Modélisation numérique en physique}\\[1.5cm] 

\rule{\linewidth}{0.5mm} \\[0.5cm]
{\LARGE \bfseries Projet 6 - Réseaux de neurones artificiels et détection du boson de Higgs} \\[0.2cm] % Titre page titre
\rule{\linewidth}{0.5mm} \\[3cm]

\large par: \\*
Patrice Béchard\\*
20019173\\[8cm] 


{\large \today}\\[3cm]

\vfill
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%								INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Dans un monde où l'internet des choses gagne de plus en plus de terrain et où une cascade continue de données est produite (données météorologiques, données sur les intérêts d'utilisateurs d'un certain produit, données en bourse, etc.), il devient impossible pour l'humain d'essayer d'analyser et de comprendre celles-ci, vu leur nombre important. Le développement d'algorithmes d'apprentissage permettant à l'ordinateur de classifier des données ou d'en extrapoler des valeurs devient primordial. L'apprentissage machine est un domaine en plein essor permettant à un ordinateur d'\textit{apprendre à apprendre}. L'une des branches de ce domaine est l'apprentissage supervisé, consistant à fournir à l'ordinateur une ensemble de données étiquetés pour lesquelles la réponse est connue. Par des techniques d'optimisation, il sera possible de minimiser la valeur d'une fonction de coût caractérisant l'écart entre la prédiction faite par l'ordinateur et la bonne réponse lui étant fournie. Il est ensuite possible de fournir à l'ordinateur un nouvel ensemble de données, cette fois-ci non étiquetée, et, si l'ordinateur est bien entraîné, pourra généraliser de ses expériences antérieures ainsi prédire la solution.

Les réseaux de neurones sont sont une catégorie d'algorithmes d'apprentissage faisant propager un signal d'entrée de noeud en noeud pour en produire une sortie. Si le résultat de la sortie s'écarte du résultat attendu, le réseau ajustera ses poids internes de sorte à produire une sortie se rapprochant de la solution attendue le plus possible. Une représentation visuelle d'un réseau de neurone artificiel est présenté à la figure \ref{fig:ffnn}. Les réseaux de neurones les plus simples sont appelés \textit{feed forward neural networks}, puisque l'information ne s'y propage que dans un sens, soit de l'entrée à la sortie, pour produire un résultat.

\begin{figure}[h]
\center
\includegraphics[scale=0.5]{figures/neural_net.jpeg}
\caption{\small{Représentation graphique d'un réseau de neurones \textit{feed forward} possédant une couche interne\cite{karpathy}.}}
\label{fig:ffnn}
\end{figure}

Il est possible d'appliquer des algorithmes d'apprentissage à plusieurs problèmes physiques. L'une des applications la plus intéressante est la détection de nouvelles particules élémentaires via les données produites par le CERN. Ce projet consiste à implémenter et entraîner un réseau de neurones artificiels pouvant détecter le boson de Higgs avec la meilleur efficacité possible. La première section de ce présent rapport présentera une validation du code implémenté en appliquant l'algorithme sur un problème plus simple. Une présentation des résultats obtenus pour la détection du boson de Higgs sera ensuite faite.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%								VALIDATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Validation du code}\label{sec:validation}

La validation du code construit a été faite en entrainant le réseau de neurones artificiel créé sur avec un échantillon de 1000 séries de 12 bits consécutifs et en le testant sur un autre échantillon de même taille. Le but était d'entraîner le réseau à pouvoir distinguer une série de 5 bits consécutifs ayant la valeur de 1. Cette même expérience a été réalisée dans les notes de cours (les bits étaient identifiés comme blanc(0) ou noir(1)) \cite{charbonneau2016notes}.

Les échantillons ont été construits en générant une suite de 12 chiffres possédant une valeur $\in \{0,1\}$ aléatoirement. Avec un algorithme \textit{force brute}, il était possible de voir si la série de bits possédait bel et bien une série de 5 bits consécutifs possédant une valeur de 1, pour ainsi pouvoir entraîner le réseau de façon supervisée. Trois fonctions d'activation distinctes ont été testées pour voir l'influence sur la sortie du réseau. Celles-ci sont la fonction \textit{logistique}, la fonction \textit{tangente hyperbolique} (tanh) et la fonction \textit{Unité de Rectification Linéaire} (ReLU) et leurs caractéristiques sont présentées au tableau \ref{tab:actv}.


\begin{table}[ht]
\center
\begin{tabular}{ c | c | c | c }
Nom & Équation & Dérivée & Codomaine \\
 \hline
logistique & $f(x) = \frac{1}{1-\mathrm{e}^{-x}}$ & $f'(x) = f(x)(1-f(x))$ & $]0,1[$ \\ 
tanh & $f(x) = \tanh(x)$ & $f'(x) = 1-f(x)^2$ & $]-\frac{\pi}{2},\frac{\pi}{2}[$ \\  
ReLU & $f(x) = \begin{cases} 0 \text{ si } x<0\\x \text{ si } x\ge 0 \end{cases}$ & $f'(x) = \begin{cases} 0 \text{ si } x<0\\1 \text{ si } x\ge 0 \end{cases}$ & $[0,\infty[$\\
\end{tabular}
\caption{\small{Caractéristiques principales des fonctions d'activation utilisées pour la confection du réseau de neurones artificiel \cite{karlik2011performance}.}}
\label{tab:actv}
\end{table}

Le réseau a été construit de sorte qu'il soit facile de modifier le nombre de noeuds dans chaque couche ainsi que d'ajouter et d'enlever des couches. Pour la vérification du code, la couche d'entrée était composée de 12 noeuds, soit le nombre de bits en entrée. La couche de sortie est composée de 2 noeuds. Pour faire la décision, le réseau prend le noeud de sortie pour lequel la valeur est maximale. Le choix du noeud 0 correspond à l'absence de 5 bits "1" consécutifs, alors que le noeud 1 correspond à la présence de ce regroupement. Le réseau ne comprends qu'une couche interne de 6 noeuds pour l'étape de la validation, ne nécessitant pas plus pour cette tâche simple.

L'évolution de la fonction de coût selon le nombre d'itérations effectuées est présentée à la figure \ref{fig:cost_actv} pour chacune des fonctions d'activation testées. Le tout a été effectué avec un paramètre d'entraînement $\alpha = 0.01$ et la méthode d'optimisation par descente de gradient. L'entraînement a été effectuée en \textit{batch} de 20 éléments, voulant dire que la descente de gradient utilise 20 échantillons pour décider de la direction vers laquelle progresser.

\begin{figure}[h]
\center
\includegraphics[scale=0.73]{figures/cost_actv.png}
\caption{\small{Évolution de la valeur de la fonction de coût selon le nombre d'itérations effectué pour chaque fonction d'activation présentée au tableau \ref{tab:actv}. Le paramètre d'entraînement a été fixé à 0.01 et la méthode de descente de gradient a été utilisée pour minimiser la fonction de coût. Les courbes présentent les valeurs pour l'échantillon d'entraînement, et les marqueurs représentent les valeurs pour l'échantillon test.}}
\label{fig:cost_actv}
\end{figure}

La différence entre les résultats produits par les différentes fonctions d'activation sont petites pour cette exemple. La fonction d'activation ReLU possède une plus grande erreur au départ, mais ceci n'est seulement dû aux configurations initiales des matrices de poids, choisis aléatoirement selon une distribution gaussienne. Toutes les fonctions d'activation convergent vers la même valeur finale, soit environ 0.05 à 5000 itérations. Le choix de la fonction d'activation dans ce cas ne semble donc pas changer énormément l'algorithme de minimisation de la fonction de coût. La fonction d'activation sigmoïdale sera utilisée pour le reste de ce projet par choix arbitraire.

La rétropropagation a été faite avec plusieurs fonctions d'optimisation pour voir laquelle était optimale. Celles-ci sont l'algorithme de descente de gradient, l'algorithme RMSProp \cite{hinton2012neural}, l'algorithme Adam \cite{kingma2014adam}, ainsi que l'algorithme AdaGrad (Gradient adaptif) \cite{duchi2011adaptive}. Ces fonctions sont directement implantées dans \textit{Tensorflow}, une bibliothèque open-source pour l'apprentissage automatique disponible sur Python. L'évolution de la fonction de coût selon le nombre d'itérations effectuées est présentée à la figure \ref{fig:cost_optimizer} pour chacune des méthodes d'optimisation étudiées. Encore une fois, le paramètre d'entraînement a été choisi à 0.01 et des \textit{batch} des 20 éléments ont été utilisées pour l'optimisation. La fonction d'activation utilisée pour cette comparaison est la fonction sigmoïdale.

\begin{figure}[h]
\center
\includegraphics[scale=0.73]{figures/cost_optimizer.png}
\caption{\small{Évolution de la valeur de la fonction de coût selon le nombre d'itérations effectué pour chaque algorithme d'optimisation étudiée. Le paramètre d'entraînement a été fixé à 0.01 et la fonction d'activation sigmoïdale a été utilisée pour cette comparaison. Les courbes présentent les valeurs pour l'échantillon d'entraînement, et les marqueurs représentent les valeurs pour l'échantillon test.}}
\label{fig:cost_optimizer}
\end{figure}

La fonction d'optimisation utilisant l'algorithme d'Adam semble être la meilleure parmi les quatre algorithmes testés pour converger rapidement vers une solution. Cependant, il faut faire attention, puisque le réseau de neurones entre rapidement en régime de surentraînement, alors que la valeur de la fonction de coût appliquée à l'échantillon d'entraînement continue de diminuer, mais que celle reliée à l'échantillon de test commence à augmenter. Le même scénario se produit pour l'algorithme RMSProp, qui converge seulement un peu plus lentement que l'algorithme d'Adam. La descente de gradient diminue selon un profil similaire à celui trouvé à la figure \ref{fig:cost_actv}, qui est cependant beaucoup plus lent que les deux algorithmes mentionnés plus tôt. Ce n'est donc pas un algorithme intéressant pour la suite des choses. Finalement, l'algorithme AdaGrad converge très lentement vers le minimum global, alors qu'à a fin de la simulation, la valeur de la fonction de coût associée aux exemplaires d'entraînement est encore nettement supérieure à celle trouvée avec les autres algorithmes. L'algorithme ne sera donc pas utilisé dans la suite des choses.

Maintenant que le réseau de neurones fonctionne bel et bien et peut apprendre d'exemples, tout en généralisant, il est possible de l'entraîner pour le problème de détection du boson de Higgs, qui constitue le sujet de ce projet. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%								RÉSULTATS ET DISCUSSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Résultats et discussion}\label{sec:results}

Après avoir vérifié que le code était bel et bien correct, il nous reste à entraîner et tester le réseau sur l'échantillon d'intérêt, soit les données du CERN pour la détection du boson de Higgs. Cependant, il n'est pas possible de seulement prendre les mêmes paramètres et la même architecture de réseau pour cette étape. La couche d'entrée du réseau possédait cette fois-ci 13 neurones, et la couche de sortie en possédait 2. Deux couches internes ont été utilisées pour l'entraînement de ce réseau. Ceux-ci ont été ajustés à 10 et 8 neurones, respectivement. Ces valeurs ont été trouvées par tâtonnement ainsi qu'en suivant quelques règles générales présentées par Jeff Heaton dans son livre \textit{Introduction to neural networks with Java}\cite{heaton2008introduction}. Une autre solution serait d'utiliser des algorithmes de \textit{pruning}, permettant de soustraire les neurones redondants dans les couches externes\cite{thimm1997pruning}. Cette méthode n'a cependant pas été implémentée pour le présent projet.

La fonctions d'activation ReLU a été utilisée pour entraîner le réseau. Ce type de fonction d'activation sont souvent utilisés dans la confection de réseaux de neurones profonds \cite{lecun2015deep}. L'algorithme d'optimisation utilisée est l'algorithme d'Adam, qui semblait fonctionner très bien lors des tests à la section précédente. 

Pour cette partie, le paramètre d'apprentissage $\alpha$ est fixé à 0.0001 de sorte que l'optimisation se face plus lentement, mais plus précisément. Avec $\alpha = 0.01$ comme à la section précédente, la valeur de la fonction de coût oscille de façon importante et le minimum est donc difficile à trouver. De plus, lorsque $\alpha = 0.001$, l'algorithme a de la difficulté à trouver le minimum, comme dans le cas précédent. La taille des \textit{batch} à passer en même temps dans le réseau est de 20, comme à la section précédente. Le réseau a été entraîné sur 50 000 itérations et a obtenu une moyenne de 75\% d'efficacité sur les différents exemplaires du fichier test. Aucun biais n'a été introduit, puisque la répartition entre les vrais positifs, faux négatifs, vrai négatifs et faux positifs était équilibrée.

Dû à la grande non-linéarité de la distribution que l'on souhaite modéliser, l'algorithme d'apprentissage a beaucoup plus de difficulté à minimiser la fonction de coût de sorte à pouvoir prédire efficacement si il y a présence du boson de Higgs sur un exemplaire à tester. La phase d'entraînement a été effectuée sur 1500 exemplaires de la base de donnée, étant interchangés aléatoirement à chacune des 50 000 itérations, de sorte que l'algorithme ne \textit{mémorise} pas les exemplaires. La minimisation de la fonction de coût est beaucoup moins lisse qu'elle ne l'était pour l'exemple de la chaine de 12 bits traitée dans la section précédente. L'évolution de cette minimisation est présentée à la figure \ref{fig:evol_cost_higgs}

\begin{figure}[h]
\center
\includegraphics[scale=0.73]{figures/higgs_cost.png}
\caption{\small{Évolution de la valeur de la fonction de coût selon le nombre d'itérations effectué pour la détection du boson de Higgs. La fonction d'activation ReLu et l'algorithme d'Adam ont été utilisés. La courbe présente les valeurs pour l'échantillon d'entraînement, et les marqueurs représentent les valeurs pour l'échantillon test.}}
\label{fig:evol_cost_higgs}
\end{figure}

On voit sur la figure qu'il aurait été judicieux d'arrêter l'entraînement vers 15000 itérations, puisqu'après, la fonction de coût pour l'échantillon test recommence à augmenter. Des résultats légèrement meilleurs auraient alors été obtenus.

À la fin de la simulation, la précision obtenue sur l'échantillon d'entraînement était d'approximativement 80\% et celle sur l'échantillon test, d'environ 75\%. Cette configuration a été préférée à celle où l'optimisation se faisait avec des \textit{batch} volumineuses (500 éléments), retournant une précision bien meilleure sur l'échantillon d'entraînement (environ 90\%), mais étant difficilement capable de généraliser, avec une précision sur l'échantillon test se trouvant entre 65\% et 70\%.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%								PROGRAMME
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Programme}
\lstinputlisting[language=Python]{codes/code_rapport_higgs.py}

\clearpage

\nocite{*}
\bibliographystyle{plain}
\bibliography{bibliographie}{}

%\begin{thebibliography}{1}
%\small
%\bibitem{notes_chap_6} Charbonneau, P., \textit{PHY3075 - Modélisation numérique en physique, Chapitre 6 : Les réseaux de neurones artificiels}, Université de Montréal, Montréal, 2017, 17p.
%\bibitem{wiki_actv} Contributeurs Wikipedia. "Activation Function." \textit{Wikipedia, The Free Encyclopedia}. Wikipedia, The Free Encyclopedia, 14 février 2017. Web. 23 avril 2017. \url{https://en.wikipedia.org/wiki/Activation_function}
%\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Annexes %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}