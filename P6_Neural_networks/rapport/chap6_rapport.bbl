\begin{thebibliography}{1}

\bibitem{charbonneau2016notes}
Paul Charbonneau.
\newblock {\em Modélisation Numérique en Physique, Notes de cours}.
\newblock Département de Physique, Université de Montréal, 2016.

\bibitem{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research}, 12(Jul):2121--2159, 2011.

\bibitem{heaton2008introduction}
Jeff Heaton.
\newblock {\em Introduction to neural networks with Java}.
\newblock Heaton Research, Inc., 2008.

\bibitem{hinton2012neural}
Geoffrey Hinton, Nirsh Srivastava, and Kevin Swersky.
\newblock Neural networks for machine learning lecture 6a overview of
  mini--batch gradient descent.
\newblock 2012.

\bibitem{karlik2011performance}
Bekir Karlik and A~Vehbi Olgac.
\newblock Performance analysis of various activation functions in generalized
  mlp architectures of neural networks.
\newblock {\em International Journal of Artificial Intelligence and Expert
  Systems}, 1(4):111--122, 2011.

\bibitem{karpathy}
Andrej Karpathy.
\newblock Neural net, 2016.
\newblock [Online; accessed April 26, 2017].

\bibitem{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em Nature}, 521(7553):436--444, 2015.

\bibitem{thimm1997pruning}
Georg Thimm and Emile Fiesler.
\newblock Pruning of neural networks.
\newblock Technical report, IDIAP, 1997.

\end{thebibliography}
