# @Author: Patrice Bechard <patricebechard>
# @Date:   2017-04-12T00:31:00-04:00
# @Email:  bechardpatrice@gmail.com
# @Last modified by:   Patrice
# @Last modified time: 2017-04-12T22:29:41-04:00
#
# Neural Network (PHY3075 - Chapter 6)

#--------------------------- Modules ------------------------------------------
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import expit         #sigmoid
import sys

filetraining = 'bitstrings_train.txt'                 #input file
filetest = 'bitstrings_test.txt'
nIter = 30000                            #number of iterations
sigmActv = True                         #which activation function to choose
nNeurons = [12,10,6,1]                     #number of neurons by layer
nLayers = len(nNeurons)                 #number of neuron layers
eta = 0.1                               #learning parameter

wMatrix = [[] for i in range(nLayers)]  #list of weight matrices

biasVector = np.array([np.zeros(nNeurons[i]) for i in range(nLayers)]) #list of bias vectors

nodeValues = [np.zeros(nNeurons[i]) for i in range(nLayers)]

#-------------------------- Functions -----------------------------------------
def actv(a):
    if sigmActv == True:
        """sigmoidal activation function (eq. 6.5)"""
        return expit(a)
    else:
        """hyperbolic tangent activation function"""
        return np.tanh(a)

def dactv(a):
    """derivative of the sigmoidal activation function (Eq. 6.19)"""
    return a * (1.-a)

def ffnn():
    """vectorized Feed-Forward neural network function"""
    for i in range(nLayers-1):
        nodeValues[i+1] = actv(biasVector[i+1] + (nodeValues[i] @ wMatrix[i]))

def backprop(err):
    """Backpropagation of the error signal and weight adjusting"""
    for i in reversed(range(1,nLayers)):
        err = np.dot(wMatrix[i],err) * dactv(nodeValues[i])
        wMatrix[i-1] += eta * (np.outer(nodeValues[i-1],err))

def init_set(f,nSample):
    """Initialize training set and answers for supervized learning"""
    tset = np.zeros([nSample,nNeurons[0]])            #training set empty array
    oset = np.zeros([nSample,nNeurons[-1]])           #answers for training set
    for i in range(nSample):
        temp = f.readline().strip().split()
        tset[i] = np.array([int(i) for i in list(temp[0])])        #input vector
        oset[i] = bool(int(temp[1]))                       #string contains 5 straight '1' bits
    return tset,oset

def training():
    """Training phase"""
    f = open(filetraining)
    nSample = int(f.readline().strip())             #number of elements in training set
    tset,oset = init_set(f,nSample)
    f.close()
    for i in range(nLayers):
        if i == nLayers-1:
            wMatrix[i] = [[1]]
        else:
            wMatrix[i] = np.random.uniform(-0.5,0.5,[nNeurons[i],nNeurons[i+1]])
    rmserr = []
    for i in range(nIter):                          #loop over all iterations
        if i%100 == 0: print(i)
        sum = 0
        order = np.random.permutation(nSample)
        for j in range(nSample):                    #loop over all element of training set
            nodeValues[0] = tset[order[j]]
            ffnn()
            err = oset[j]-nodeValues[-1]
            sum += np.sqrt(err*err)
            backprop(err)
        rmserr.append(np.sqrt(sum/(nSample*nNeurons[-1])))   #rms error for this iteration
    plt.semilogx(np.arange(nIter),rmserr,'k.')
    plt.show()

def save_network():
    """Save network after training"""
    g = open('saved_network.txt','w')
    numNodes = ''
    for i in range(nLayers):
        numNodes += str(nNeurons[i])
        if i != nLayers-1:
            numNodes += ' '
        else:
            numNodes += '\n'
    g.write(numNodes)
    for i in range(nLayers):
        for line in wMatrix[i]:
            for elem in line:
                g.write(str(elem)+' ')
        if i != nLayers-1:
            g.write('\n')
    g.close()

def load_network():
    g = open('saved_network.txt')
    nNeurons = g.readline().strip().split()
    nNeurons = [int(i) for i in nNeurons]
    nLayers = len(nNeurons)
    for i in range(nLayers):
        data = g.readline().strip().split()
        data = [float(i) for i in data]
        if i != nLayers-1:
            data = np.array([data]).reshape(nNeurons[i],nNeurons[i+1])
        wMatrix[i] = data
    g.close()

def test():
    f = open(filetest)
    nSample = int(f.readline().strip())
    testset,oset = init_set(f,nSample)
    f.close()
    g = open('results.txt','w')
    ratio = 0
    for i in range(nSample):
        result = False
        nodeValues[0] = testset[i]
        ffnn()
        if float(nodeValues[-1]) > 0.5:
            result = True
        if result == oset[i]:
            ratio += 1
            g.write('1\n')
        else:
            continue
            g.write('0\n')
    g.close()
    print("SUCCESS RATIO : %f"%(ratio/nSample))

#----------------------------- Main -------------------------------------------
if 'train' in sys.argv:
    training()
    save_network()
if 'test' in sys.argv:
    if not 'train' in sys.argv:
        load_network()
    test()
